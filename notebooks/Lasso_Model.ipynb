{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Import necessary libraries \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "import sys\n",
    "# adding to the path variables the one folder higher (locally, not changing system variables)\n",
    "sys.path.append(\"..\")\n",
    "import pandas as pd\n",
    "# from pandas_profiling import ProfileReport\n",
    "import numpy as np\n",
    "import warnings\n",
    "import mlflow\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from modeling.config import EXPERIMENT_NAME\n",
    "TRACKING_URI = open(\"../.mlflow_uri\").read().strip()\n",
    "\n",
    "ROOT = os.environ.get('PWD')\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# save to csv if desired: this is the normed, filtered set with agg price data for the years in question\n",
    "master_dummies = pd.read_csv('../data/master_with_dummies.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modelling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# free up memory\n",
    "del master_filter_1\n",
    "gc.collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create X and Y sets for train test split\n",
    "X = master_dummies.drop(['listing_id','inquiry_count'],axis=1)\n",
    "Y = master_dummies['inquiry_count']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, random_state=42)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "the_list = ['mean_price_per_day','adult_count','children_count','pets_count','length_stay','bathrooms','bedrooms','max_guests','living_area']"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Scaling with MinMaxScaler\n",
    "scaler_norm = MinMaxScaler()\n",
    "X_train_scaled_norm = scaler_norm.fit_transform(X_train[the_list])\n",
    "X_test_scaled_norm = scaler_norm.transform(X_test[the_list])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Concatenating normalized columns \n",
    "X_train_preprocessed_norm = np.concatenate([X_train_scaled_norm, X_train.drop(the_list, axis=1)], axis=1)\n",
    "X_test_preprocessed_norm = np.concatenate([X_test_scaled_norm, X_test.drop(the_list, axis=1)], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train_preprocessed_norm.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_test_preprocessed_norm.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Trainining the model and tracking with MLFlow\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "mlflow.start_run()\n",
    "run = mlflow.active_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#training the model\n",
    "reg1 = LinearRegression().fit(X_train_preprocessed_norm, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_train_pred = reg1.predict(X_train_preprocessed_norm)\n",
    "rmse_train = mean_squared_error(y_train, y_train_pred,squared=False)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "print(rmse_train)\n",
    "print(r2_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_test_pred = reg1.predict(X_test_preprocessed_norm)\n",
    "rmse_test = mean_squared_error(y_test, y_test_pred,squared=False)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "print(rmse_test)\n",
    "print(r2_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Validating model and visualizing model and residuals"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mean_absolute_error(y_test, y_test_pred)\n",
    "print(cross_val_score(reg1, X, Y, cv=10, scoring='r2').mean())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_residual = y_test - y_test_pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.scatterplot(x=X_train, y=y_rep, )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.rcParams['agg.path.chunksize'] = 10000"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.residplot(x=y_test, y=y_test_pred)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.scatter(y=y_test, x=y_test_pred)\n",
    "plt.plot(y_test_pred, y_test_pred, color='orange')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('predicted')\n",
    "plt.title(f'Actual VS Predicted Inquiries')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.axhline(0, c=(.5, .5, .5), ls='--')\n",
    "plt.axvline(0, c=(.5, .5, .5), ls='--')\n",
    "plt.scatter(x=y_test_pred, y=y_residual)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.displot(y_residual)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "params = {\n",
    "      \"Model\" : \"Lasso\",\n",
    "      \"Folds this run\": 5\n",
    "      \"train_test_split\": 30,\n",
    "      \"normalized data\": 'yes',\n",
    "      \"2019 and 2020 data\": \"2019\", \n",
    "      \"metric\": 'rmse', 'r2'\n",
    "      \"cross_val_score\": \"yes\", \"no\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lflow.log_params(params)\n",
    "mlflow.set_tag(\"running_from_jupyter\", \"Lasso model 2019\")\n",
    "mlflow.log_metric(\"train -\" + \"RMSE\", rmse_train)\n",
    "mlflow.log_metric(\"test -\" + \"RMSE\", rmse_test)\n",
    "# mlflow.log_artifact(\"../models\")\n",
    "# mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.get_run(run_id=run.info.run_id)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Checking the experiments"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# while the next cell is running you will not be able to run other cells in the notebook\n",
    "!mlflow ui"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('.venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  },
  "interpreter": {
   "hash": "3fd22edfe25832d446c332425401e3196937690f846d01f80d16cc13753e72c7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}