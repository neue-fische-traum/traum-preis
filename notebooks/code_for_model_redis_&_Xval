# To add a new cell, type '# %%'
# To add a new markdown cell, type '# %% [markdown]'
# %%
from IPython import get_ipython

# %% [markdown]
# # Code for residuals and validations notebooks
# %% [markdown]
# ## Import necessary libraries 

# %%
import sys
# adding to the path variables the one folder higher (locally, not changing system variables)
sys.path.append("..")
import pandas as pd
# from pandas_profiling import ProfileReport
import numpy as np
import warnings
import mlflow
import math 
import seaborn as sns
import matplotlib.pyplot as plt 
from matplotlib import rc_params
from scipy.stats import pearsonr
import statsmodels.api as sm
from statsmodels.formula.api import ols
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error 
from sklearn.model_selection import cross_val_score 
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from modeling.config import EXPERIMENT_NAME
TRACKING_URI = open("../.mlflow_uri").read().strip()

ROOT = os.environ.get('PWD')

warnings.filterwarnings('ignore')




# %% [markdown]
# ## Trainining the model and tracking with MLFlow

# %%
# setting the MLFlow connection and experiment
mlflow.set_tracking_uri(TRACKING_URI)
mlflow.set_experiment(EXPERIMENT_NAME)
mlflow.start_run()
run = mlflow.active_run()


# %%
print("Active run_id: {}".format(run.info.run_id))


# %%
#training the model
reg1 = LinearRegression().fit(X_train_preprocessed_norm, y_train)


# %%
y_train_pred = reg1.predict(X_train_preprocessed_norm)
rmse_train = mean_squared_error(y_train, y_train_pred,squared=False)
r2_train = r2_score(y_train, y_train_pred)
print(rmse_train)
print(r2_train)


# %%
y_test_pred = reg1.predict(X_test_preprocessed_norm)
rmse_test = mean_squared_error(y_test, y_test_pred,squared=False)
r2_test = r2_score(y_test, y_test_pred)
print(rmse_test)
print(r2_test)

# %% [markdown]
# ## Validating model and visualizing model and residuals

# %%
mean_absolute_error(y_test, y_test_pred)


# %%
print(cross_val_score(reg1, X, Y, cv=10, scoring='r2').mean())


# %%
y_residual = y_test - y_test_pred


# %%
sns.scatterplot(x=X_train, y=y_rep, )


# %%
plt.rcParams['agg.path.chunksize'] = 10000


# %%
sns.residplot(x=y_test, y=y_test_pred)


# %%

plt.scatter(y=y_test, x=y_test_pred)
plt.plot(y_test_pred, y_test_pred, color='orange')
plt.xlabel('Actual')
plt.ylabel('predicted')
plt.title(f'Actual VS Predicted Inquiries')
plt.show()


# %%
plt.axhline(0, c=(.5, .5, .5), ls='--')
plt.axvline(0, c=(.5, .5, .5), ls='--')
plt.scatter(x=y_test_pred, y=y_residual)


# %%
sns.displot(y_residual)


# %%
params = {
      "Model" : "Lasso",
      "Folds this run": 5
      "train_test_split": 30,
      "normalized data": 'yes',
      "2019 and 2020 data": "2019", 
      "metric": 'rmse', 'r2'
      "cross_val_score": "yes", "no"
  }


# %%
lflow.log_params(params)
mlflow.set_tag("running_from_jupyter", "Lasso model 2019")
mlflow.log_metric("train -" + "RMSE", rmse_train)
mlflow.log_metric("test -" + "RMSE", rmse_test)
# mlflow.log_artifact("../models")
# mlflow.sklearn.log_model(reg, "model")
mlflow.end_run()


# %%
mlflow.get_run(run_id=run.info.run_id)

# %% [markdown]
# ## Checking the experiments
# 
# while the next cell is running you will not be able to run other cells in the notebook

# %%
get_ipython().system('mlflow ui')


